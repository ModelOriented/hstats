% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pd.R
\name{pd}
\alias{pd}
\alias{pd.default}
\alias{pd.ranger}
\alias{pd.Learner}
\title{Partial Dependence Profiles}
\usage{
pd(object, ...)

\method{pd}{default}(
  object,
  v,
  X,
  pred_fun = stats::predict,
  grid = NULL,
  grid_size = 36L,
  trim = c(0.01, 0.99),
  strategy = c("quantile", "uniform"),
  n_max = 1000L,
  w = NULL,
  ...
)

\method{pd}{ranger}(
  object,
  v,
  X,
  pred_fun = function(m, X, ...) stats::predict(m, X, ...)$predictions,
  grid = NULL,
  grid_size = 36L,
  trim = c(0.01, 0.99),
  strategy = c("quantile", "uniform"),
  n_max = 1000L,
  w = NULL,
  ...
)

\method{pd}{Learner}(
  object,
  v,
  X,
  pred_fun = function(m, X) m$predict_newdata(X)$response,
  grid = NULL,
  grid_size = 36L,
  trim = c(0.01, 0.99),
  strategy = c("quantile", "uniform"),
  n_max = 1000L,
  w = NULL,
  ...
)
}
\arguments{
\item{object}{Fitted model object.}

\item{...}{Additional arguments passed to \code{pred_fun(object, X, ...)}, for instance
\code{type = "response"} in a \code{\link[=glm]{glm()}} model.}

\item{v}{Vector of feature names.}

\item{X}{A data.frame or matrix serving as background dataset.}

\item{pred_fun}{Prediction function of the form \verb{function(object, X, ...)},
providing K >= 1 numeric predictions per row. Its first argument represents the
model \code{object}, its second argument a data structure like \code{X}. Additional arguments
(such as \code{type = "response"} in a GLM) can be passed via \code{...}. The default,
\code{\link[stats:predict]{stats::predict()}}, will work in most cases. Note that column names in a resulting
matrix of predictions will be used as default column names in the results.}

\item{grid}{Named list. Each element specifies the evaluation grid for the
corresponding feature. Missing components are automatically generated via
\code{\link[=univariate_grid]{univariate_grid()}}. If \code{v} has length 1, then \code{grid} can also be a vector.}

\item{grid_size}{Controls the approximate grid size. If \code{x} has p columns, then each
(non-discrete) column will be reduced to about the p-th root of \code{grid_size} values.}

\item{trim}{A non-discrete numeric \code{z} is trimmed at these quantile probabilities
before calculations. Set to \code{c(0, 1)} for no trimming.}

\item{strategy}{How to find evaluation points of non-discrete numeric columns?
Either "quantile" or "uniform" (via \code{\link[=pretty]{pretty()}}), see description of
\code{\link[=univariate_grid]{univariate_grid()}}.}

\item{n_max}{If \code{X} has more than \code{n_max} rows, a random sample of \code{n_max} rows is
selected from \code{X}. In this case, set a random seed for reproducibility.}

\item{w}{Optional vector of case weights for each row of \code{X}.}
}
\value{
A list of PD profiles per variable in \code{v}. Has additional class "pd".
}
\description{
Estimates the partial dependence function of features \code{v} over a
grid of values.
}
\section{Methods (by class)}{
\itemize{
\item \code{pd(default)}: Default method.

\item \code{pd(ranger)}: Method for "ranger" models.

\item \code{pd(Learner)}: Method for "mlr3" models.

}}
\section{Partial Dependence Functions}{


Let \eqn{F: R^p \to R} denote the prediction function that maps the
\eqn{p}-dimensional feature vector \eqn{\mathbf{x} = (x_1, \dots, x_p)}
to its prediction. Furthermore, let
\deqn{
  F_s(\mathbf{x}_s) = E_{\mathbf{x}_{\setminus s}}(F(\mathbf{x}_s, \mathbf{x}_{\setminus s}))
}
be the partial dependence function of \eqn{F} on the feature subset
\eqn{\mathbf{x}_s}, where \eqn{s \subseteq \{1, \dots, p\}}, as introduced in
Friedman (2001). Here, the expectation runs over the joint marginal distribution
of features \eqn{\mathbf{x}_{\setminus s}} not in \eqn{\mathbf{x}_s}.

Given data, \eqn{F_s(\mathbf{x}_s)} can be estimated by the empirical partial
dependence function

\deqn{
  \hat F_s(\mathbf{x}_s) = \frac{1}{n} \sum_{i = 1}^n F(\mathbf{x}_s, \mathbf{x}_{i\setminus s}),
}
where \eqn{\mathbf{x}_{i\setminus s}} \eqn{i = 1, \dots, n}, are the observed values
of \eqn{\mathbf{x}_{\setminus s}}.
}

\examples{
# MODEL ONE: Linear regression
fit <- lm(Sepal.Length ~ ., data = iris)
pd <- pd(fit, v = "Species", X = iris)
pd$pd
head(pd$pd)

pd <- pd(fit, v = "Petal.Width", X = iris, grid = seq(1, 0, by = -0.5))
pd$pd

# MODEL TWO: Multi-response linear regression
fit <- lm(as.matrix(iris[1:2]) ~ Petal.Length + Petal.Width + Species, data = iris)
pd <- pd(fit, v = "Petal.Width", X = iris)
pd$pd[1:4, ]
 
# MODEL THREE: Gamma GLM -> pass options to predict() via ...
fit <- glm(
  Sepal.Length ~ . + Petal.Width:Species, 
  data = iris, 
  family = Gamma(link = log)
)
pd(fit, v = "Species", X = iris, type = "response")$pd
}
\references{
Friedman, Jerome H. \emph{"Greedy Function Approximation: A Gradient Boosting Machine."}
Annals of Statistics 29, no. 5 (2001): 1189-1232.
}

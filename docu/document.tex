\documentclass[]{article}

%opening
\title{}
\author{}

\begin{document}

\section{Background}
\subsection{Partial dependence}
Let $F: R^p \to R$ denote the prediction function that maps the $p$-dimensional feature vector $\mathbf{x} = (x_1, \dots, x_p)$ to its prediction.
Furthermore, let $F_s(\mathbf{x}_s) = E_{\mathbf{x}_{\setminus s}}(F(\mathbf{x}_s, \mathbf{x}_{\setminus s}))$ be the partial dependence function of $F$ on the feature subset $\mathbf{x}_s$, where $s \subseteq \{1, \dots, p\}$, as introduced in [1]. Here, the expectation runs over the joint marginal distribution of features $\mathbf{x}_{\setminus s}$ not in $\mathbf{x}_s$.

Given data, $F_s(\mathbf{x}_s)$ can be estimated by the empirical partial dependence function
$$
  \hat F_s(\mathbf{x}_s) = \frac{1}{n} \sum_{i = 1}^n F(\mathbf{x}_s, \mathbf{x}_{i\setminus s}),
$$
where $\mathbf{x}_{i\setminus s}$, $i = 1, \dots, n$, are the observed values of $\mathbf{x}_{\setminus s}$.

\subsection{Overall interaction strength}
In [2], Friedman and Popescu introduced different statistics to measure interaction strength. Closely following their notation, we will summarize the main ideas. 

If there are no interactions involving $x_j$, we can decompose the prediction function $F$ as the sum of the partial dependence $F_j$ on $x_j$ and the partial dependence $F_{\setminus j}$ on all other features $\mathbf{x}_{\setminus j}$, i.e.,
$$
	F(\mathbf{x}) = F_j(x_j) + F_{\setminus j}(\mathbf{x}_{\setminus j}).
$$
Correspondingly, Friedman and Popescu's $H^2_j$ statistic of overall interaction strength is given by
$$
	H_{j}^2 = \frac{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i) - \hat F_j(x_{ij}) - \hat F_{\setminus j}(\mathbf{x}_{i\setminus k})\big]^2}{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i)\big]^2}.
$$
\paragraph{Remarks}
\begin{enumerate}
	\item Partial dependence functions are all centered to mean 0.
	\item Partial dependence functions are evaluated over the data distribution. This is different to partial dependence plots, where one uses a fixed grid.
	\item Weighted versions follow by replacing all arithmetic means by corresponding weighted means.
	\item Multivariate predictions can be treated in a component-wise manner.
	\item $H^2_j = 0$ means there are no interactions associated with $x_j$. The higher the value, the more prediction variability comes from interactions with $x_j$.
	\item Since the denominator is the same for all features, the values of the test statistics can be compared across features.
\end{enumerate}

\subsection{Pairwise interaction strength}
Again following [2], if there are no interaction effects between features $x_j$ and $x_k$, their two-dimensional partial dependence function $F_{jk}$ can be written as the sum of the univariate partial dependencies, i.e.,
$$
  F_{jk}(x_j, x_k) = F_j(x_j)+ F_k(x_k).
$$
Correspondingly, Friedman and Popescu's $H_{jk}^2$ statistic of pairwise interaction strength is defined as
$$
  H_{jk}^2 = \frac{A_{jk}}{B_{jk}},
$$
where 
$$
  A_{jk} = \frac{1}{n} \sum_{i = 1}^n\big[\hat F_{jk}(x_{ij}, x_{ik}) - \hat F_j(x_{ij}) - \hat F_k(x_{ik})\big]^2
$$
and
$$
  B_{jk} = \frac{1}{n} \sum_{i = 1}^n\big[\hat F_{jk}(x_{ij}, x_{ik})\big]^2.
$$
\paragraph{Remarks}
\begin{enumerate}
	\item Remarks 1--4 of $H^2_{j}$ also apply here.
	\item $H^2_{jk} = 0$ means there are no interaction effects between $x_j$ and $x_k$. The larger the value, the more of the joint effect of the two features comes from the interaction.
	\item Since the denominator differs between variable pairs, unlike $H_j$, this test statistic is difficult to compare between variable pairs. If both main effects are very weak, a negligible interaction can get a high $H^2_{jk}$. Therefore, [2] suggests to calculate $H^2_{jk}$ only for {\em important} variables.
\end{enumerate}

\paragraph{Alternatives:} To be able to compare pairwise interaction strength across variable pairs, and to overcome the problem mentioned in the last remark, we suggest as alternative a different denominator, namely the same as used for $H^2_j$:
$$
  \tilde H^2_{jk} = \frac{A_{jk}}{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i)\big]^2}.
$$
This statistic would tell us how much of the total variance of the predictions comes from the pairwise interaction of $x_j$ and $x_k$.

Another possibility would be to use the unnormalized test statistic on the scale of the predictions, i.e., $\sqrt{A_{jk}}$.

\subsection{Total interaction strength of all variables together}
In the spirit of [2], we can say: if the model is additive in all features (no interactions), then
$$
	F(\mathbf{x}) = \sum_{j}^{p} F_j(x_j).
$$
To measure the relative amount of variability explained by all interactions, we can therefore study the test statistic of total interaction strength
$$
  H^2 = \frac{\frac{1}{n} \sum_{i = 1}^n \big[F(\mathbf{x}_i) - \sum_{j = 1}^p\hat F_j(x_{ij})\big]^2}{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i)\big]^2}.
$$
It equals the variability of the predictions unexplained by the main effects. A value of 0 would mean there are no interaction effects at all.

\subsection{Workflow}
Calculation of all $H_j^2$ statistics requires $O(n^2p)$ predictions, while calculating of all pairwise $H_{jk}$ requires $O(n^2 p^2)$ predictions. Therefore, we suggest to reduce the workflow in two important ways:
\begin{itemize}
\item Evaluate the statistics only on a subset of the data, e.g., on $n' = 400$ observations.
\item Calculate $H_j^2$ for all features. Then, select a small number $m = o(\sqrt{p})$ of features with highest $H^2_j$ and do pairwise calculations only on this subset.
\end{itemize}
This leads to a total number of $O(n'^2 p))$ predictions.
\end{document}

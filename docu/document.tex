\documentclass[]{article}

%opening
\title{}
\author{}

\begin{document}

\section{Partial dependence}
Let $F: R^p \to R$ denote the prediction function that maps the $p$-dimensional feature vector $\mathbf{x} = (x_1, \dots, x_p)$ to its prediction.
Furthermore, let $F_s(\mathbf{x}_s) = E_{\mathbf{x}_{\setminus s}}(F(\mathbf{x}_s, \mathbf{x}_{\setminus s}))$ be the partial dependence function of $F$ on the feature subset $\mathbf{x}_s$, where $s \subseteq \{1, \dots, p\}$, as introduced in \cite{friedman2001}. Here, the expectation runs over the joint marginal distribution of features $\mathbf{x}_{\setminus s}$ not in $\mathbf{x}_s$.

Given data, $F_s(\mathbf{x}_s)$ can be estimated by the empirical partial dependence function
$$
  \hat F_s(\mathbf{x}_s) = \frac{1}{n} \sum_{i = 1}^n F(\mathbf{x}_s, \mathbf{x}_{i\setminus s}),
$$
where $\mathbf{x}_{i\setminus s}$, $i = 1, \dots, n$, are the observed values of $\mathbf{x}_{\setminus s}$. Its disaggregated version is called
{\em individual conditional expectation} (ICE), see \cite{goldstein2015}.

\section{Interaction statistics}

\subsection{Overall interaction strength}
In \cite{friedman2008}, Friedman and Popescu introduced different statistics to measure interaction strength. Closely following their notation, we will summarize the main ideas. 

If there are no interactions involving $x_j$, we can decompose the prediction function $F$ into the sum of the partial dependence $F_j$ on $x_j$ and the partial dependence $F_{\setminus j}$ on all other features $\mathbf{x}_{\setminus j}$, i.e.,
$$
	F(\mathbf{x}) = F_j(x_j) + F_{\setminus j}(\mathbf{x}_{\setminus j}).
$$
Correspondingly, Friedman and Popescu's $H^2_j$ statistic of overall interaction strength is given by
$$
	H_{j}^2 = \frac{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i) - \hat F_j(x_{ij}) - \hat F_{\setminus j}(\mathbf{x}_{i\setminus j})\big]^2}{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i)\big]^2}.
$$
\paragraph{Remarks}
\begin{enumerate}
	\item Partial dependence functions (and $F$) are all centered to mean 0.
	\item Partial dependence functions (and $F$) are evaluated over the data distribution. This is different to partial dependence plots, where one uses a fixed grid.
	\item Weighted versions follow by replacing all arithmetic means by corresponding weighted means.
	\item Multivariate predictions can be treated in a component-wise manner.
	\item Due to (typically undesired) extrapolation effects, depending on the model, values above 1 may occur.
	\item $H^2_j = 0$ means there are no interactions associated with $x_j$. The higher the value, the more prediction variability comes from interactions with $x_j$.
	\item Since the denominator is the same for all features, the values of the test statistics can be compared across features.
\end{enumerate}

\subsection{Pairwise interaction strength}
Again following \cite{friedman2008}, if there are no interaction effects between features $x_j$ and $x_k$, their two-dimensional partial dependence function $F_{jk}$ can be written as the sum of the univariate partial dependencies, i.e.,
$$
  F_{jk}(x_j, x_k) = F_j(x_j)+ F_k(x_k).
$$
Correspondingly, Friedman and Popescu's $H_{jk}^2$ statistic of pairwise interaction strength is defined as
$$
  H_{jk}^2 = \frac{A_{jk}}{B_{jk}},
$$
where 
$$
  A_{jk} = \frac{1}{n} \sum_{i = 1}^n\big[\hat F_{jk}(x_{ij}, x_{ik}) - \hat F_j(x_{ij}) - \hat F_k(x_{ik})\big]^2
$$
and
$$
  B_{jk} = \frac{1}{n} \sum_{i = 1}^n\big[\hat F_{jk}(x_{ij}, x_{ik})\big]^2.
$$
\paragraph{Remarks}
\begin{enumerate}
	\item Remarks 1--5 of $H^2_{j}$ also apply here.
	\item $H^2_{jk} = 0$ means there are no interaction effects between $x_j$ and $x_k$. The larger the value, the more of the joint effect of the two features comes from the interaction.
	\item Since the denominator differs between variable pairs, unlike $H_j$, this test statistic is difficult to compare between variable pairs. If both main effects are very weak, a negligible interaction can get a high $H^2_{jk}$. Therefore, \cite{friedman2008} suggests to calculate $H^2_{jk}$ only for {\em important} variables.
\end{enumerate}

\paragraph{Modification:} To be better able to compare pairwise interaction strength across variable pairs, and to overcome the problem mentioned in the last remark, we suggest as alternative the unnormalized test statistic on the scale of the predictions, i.e., $\sqrt{A_{jk}}$. 
Furthermore, we do pairwise calculations not for the most {\em important} features but rather for those features with {\em strongest overall interactions}.

\subsection{Three-way interactions}
\cite{friedman2008} also describes a test statistic to measure three-way interactions: in case there are no three-way interactions between features $x_j$, $x_k$ and $x_l$, their three-dimensional partial dependence function $F_{jkl}$ can be decomposed into lower order terms:
$$
  F_{jkl}(x_j, x_k, x_l) = F_{jk}(x_j, x_k) + F_{jl}(x_j, x_l) + F_{kl}(x_k, x_l) - F_j(x_j) - F_k(x_k) - F_l(x_l).
$$
The squared and scaled difference between the two sides of the equation leads to the statistic
$$
  H_{jkl}^2 = \frac{\frac{1}{n} \sum_{i = 1}^n \big[\hat F_{jkl}(x_j, x_k, x_l) - C_{jkl}\big]^2}{\frac{1}{n} \sum_{i = 1}^n \hat F_{jkl}(x_j, x_k, x_l)^2},
$$
where
$$
	C_{jkl} = \hat F_{jk}(x_j, x_k) + \hat F_{jl}(x_j, x_l) + \hat F_{kl}(x_k, x_l) - \hat F_j(x_j) - \hat F_k(x_k) - \hat F_l(x_l).
$$
Similar remarks as for $H_{jk}$ apply.

\subsection{Total interaction strength of all variables together}
If the model is additive in all features (no interactions), then
$$
	F(\mathbf{x}) = \sum_{j}^{p} F_j(x_j),
$$
i.e., the (centered) predictions can be written as the sum of the (centered) main effects.
To measure the relative amount of variability unexplained by all main effects, we can therefore study the test statistic of total interaction strength
$$
  H^2 = \frac{\frac{1}{n} \sum_{i = 1}^n \big[F(\mathbf{x}_i) - \sum_{j = 1}^p\hat F_j(x_{ij})\big]^2}{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i)\big]^2}.
$$
A value of 0 means there are no interaction effects at all. Due to (typically undesired) extrapolation effects, depending on the model, values above 1 may occur.

In \cite{zolkowski2023}, $1 - H^2$ is called {\em additivity index}. A similar measure using accumulated local effects is discussed in \cite{molnar2020}.

\subsection{Workflow}
Calculation of all $H_j^2$ statistics requires $O(n^2p)$ predictions, while calculating of all pairwise $H_{jk}$ requires $O(n^2 p^2)$ predictions. Therefore, we suggest to reduce the workflow in two important ways:
\begin{itemize}
\item Evaluate the statistics only on a subset of the data, e.g., on $n' = 300$ observations.
\item Calculate $H_j^2$ for all features. Then, select a small number $m = O(\sqrt{p})$ of features with highest $H^2_j$ and do pairwise calculations only on this subset.
\end{itemize}
This leads to a total number of $O(n'^2 p))$ predictions. If also three-way interactions are to be studied, $m$ should be of the order $p^{1/3}$.

\section{Variable importance}

\cite{greenwell2018} proposed the standard deviation of the partial dependence function as a measure of variable importance (for continuous predictors). 

Since the partial dependence function suppresses interaction effects, we propose a different measure in the spirit of the interaction statistics above: If $x_j$ has no effects, the (centered) prediction function $F$ equals the (centered) partial dependence $F_{\setminus j}$ on all other features $\mathbf{x}_{\setminus j}$, i.e.,
$$
	F(\mathbf{x}) = F_{\setminus j}(\mathbf{x}_{\setminus j}).
$$
Therefore, the following measure of variable importance follows:
$$
	\textrm{Imp}_j = \frac{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i) - \hat F_{\setminus j}(\mathbf{x}_{i\setminus j})\big]^2}{\frac{1}{n} \sum_{i = 1}^n\big[F(\mathbf{x}_i)\big]^2}.
$$
It differs from $H^2_j$ only by not subtracting the main effect of the $j$-th feature in the numerator. It can be read as the proportion of prediction variability unexplained by all other features. As such, it measures variable importance of the $j$-th feature, including its interaction effects. 

\bibliographystyle{ieeetr}
\bibliography{biblio}

\end{document}
